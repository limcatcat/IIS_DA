{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practice Choosing Models"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, KFold, cross_val_score # to split the data\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, fbeta_score #To evaluate our model\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Algorithmns models to be compared\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "df = pd.read_csv('german_credit_data.csv', index_col=0)\n",
    "data = load_breast_cancer()\n",
    "df2 = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "df2['target'] = data.target"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intro\n",
    "\n",
    "In the german_credit_data dataset we are trying to predict if an individual is credit worthy or not by classifying an entry as good or bad in the 'Risk' value\n",
    "\n",
    "In the second dataset we are trying to predict if a patient has breast cancer, which is labeld in the 'target' column."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_dataset(test_val, df):   \n",
    "    # Split dataset into training and testing sets\n",
    "    X = df.drop(test_val, axis=1)\n",
    "    y = df[test_val]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "    # Scale the features using StandardScaler\n",
    "    sc = StandardScaler()\n",
    "    X_train = sc.fit_transform(X_train)\n",
    "    X_test = sc.transform(X_test)\n",
    "\n",
    "    models = []\n",
    "    models.append(('LR', LogisticRegression()))\n",
    "    models.append(('LDA', LinearDiscriminantAnalysis()))\n",
    "    models.append(('KNN', KNeighborsClassifier()))\n",
    "    models.append(('CART', DecisionTreeClassifier()))\n",
    "    models.append(('NB', GaussianNB()))\n",
    "    models.append(('RF', RandomForestClassifier()))\n",
    "    models.append(('SVM', SVC(gamma='auto')))\n",
    "\n",
    "    # evaluate each model in turn\n",
    "    results = []\n",
    "\n",
    "    for name, model in models:\n",
    "            model.fit(X_train, y_train)\n",
    "            y_pred = model.predict(X_test)\n",
    "            results.append(y_pred)\n",
    "            msg = \"%s: %f\" % (name, accuracy_score(y_test,y_pred))\n",
    "            print(msg)\n",
    "    print(\"\\n\")\n",
    "    return [y_test, results] # Returns all accuracy scores of the single models"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1) Data Overview\n",
    "\n",
    "Take a look at both datasets. What values are you working with? Also think about which models could be suitable for each dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2) Data Cleanup\n",
    "\n",
    "Try to replace all missing values with the means of their according columns and convert all non numeric values into numeric values in the german_credit_dataset. Can you also come up with a new feature to improve your result?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3) Find a suitable Model\n",
    "\n",
    "Try your dataset with the given test_dataset function and interpret the results. Which ml-learning model is the best suited one for each problem?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4) Confusion Matrix\n",
    "\n",
    "Give out a confusion Matrix with the models you have choosen and interpret the results. Why does one model and dataset perform better than the other?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
